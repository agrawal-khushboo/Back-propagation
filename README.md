This module is an illustration to classify the MNIST using multi layer neural networks with softmax inputs.

we have used vectorized update rule to perform gradient descent to learn the classifier which maps the imput data to labels using one -hot encoding. 50 input units are considered
Added weight decay to the update rule with regularization constane lamda.
The following code has 3 modules for different activation functions as sigmoid, ReLU, tanh
Further improvization on the code can be made by adding the hidden layer on the configuration .
